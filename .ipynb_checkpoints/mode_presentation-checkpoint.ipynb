{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the packages we will require, notably TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set our hyperparameters. The basic structure is very similar to that in Appendix XYZ1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_epochs = 50000\n",
    "batch_size = 1000\n",
    "hidden_layer_size_d = 6\n",
    "hidden_layer_size_g = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this script to carry out simulations for both Sections XYZ2 and XYZ3. We randomly select which objective function we are going to use, and whether or not we are going to be using minibatch discirmination. Then when this script is iterated many times we collect data across all four combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_objective = np.random.choice((1, 2), 1)[0]\n",
    "minibatch_discrimination = np.random.choice((True, False), 1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the target distribution, which is mixture of two Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_mean_1 = 0\n",
    "real_sd_1 = 1\n",
    "\n",
    "real_mean_2 = 10\n",
    "real_sd_2 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated in Section XYZ2XYZ4 we are using a very simple form of minibatch discirmination in which the discriminator is allowed access to the minibatch mean and range. To calculate the range we create the function __minibatch_l1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_l1(input):\n",
    "    maxi = tf.reduce_max(input)\n",
    "    mini = tf.reduce_min(input)\n",
    "    return tf.subtract(maxi,mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the discriminator. As this script is going to run both with and without minbatch discrimination we create two discriminators, one for each case. The discriminator with minibacth discrimination is defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(input, parameters, max_batch_dist, batch_mean):\n",
    "    pre_1 = tf.add(tf.matmul(tf.to_float(input), parameters[0]), parameters[1])\n",
    "    activ_1 = tf.tanh(pre_1)\n",
    "    pre_2 = tf.add(tf.matmul(activ_1, parameters[2]), parameters[3])\n",
    "    activ_2 = tf.tanh(pre_2)\n",
    "    pre_3 = tf.add(tf.matmul(activ_2, parameters[4]), parameters[5])\n",
    "\n",
    "    mini_1 = tf.add(tf.multiply(max_batch_dist,parameters[6]),parameters[7])\n",
    "\n",
    "    mean_1 = tf.add(tf.multiply(batch_mean,parameters[8]),parameters[9])\n",
    "\n",
    "    mixed1 = tf.add(tf.multiply(mini_1,parameters[10]),pre_3)\n",
    "    mixed2 = tf.add(tf.multiply(mean_1,parameters[11]),mixed1)\n",
    "    output = tf.sigmoid(mixed2)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first five lines of the function, the discriminator is processing the individual sample as usual. In the following two lines we multiply the minibatch mean and range by their own weights and biases. This gives us three numbers, __pre_3__, __mini_1__ and __mean_1__. We then take a weighted average of these three and pass it through a sigmoid function to get the output. The discriminator without minibatch discrimination we call __simple_deiscimrinator__ and takes the same form as the discriminator in Appendix XYZ1. We also define the generator which again is the same form as that which was used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_discriminator(input, parameters):\n",
    "    pre_1 = tf.add(tf.matmul(tf.to_float(input), parameters[0]), parameters[1])\n",
    "    activ_1 = tf.tanh(pre_1)\n",
    "    pre_2 = tf.add(tf.matmul(activ_1, parameters[2]), parameters[3])\n",
    "    activ_2 = tf.tanh(pre_2)\n",
    "    pre_3 = tf.add(tf.matmul(activ_2, parameters[4]), parameters[5])\n",
    "    output1 = tf.sigmoid(pre_3)\n",
    "    return output1\n",
    "\n",
    "\n",
    "def generator(input, parameters):\n",
    "    pre_1 = tf.add(tf.matmul(tf.to_float(input), parameters[0]), parameters[1])\n",
    "    activ_1 = tf.tanh(pre_1)\n",
    "    output = tf.add(tf.matmul(activ_1, parameters[2]), parameters[3])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to define all the nessescary parameters, first for $D$. Note the parameters for minibatch discirminatoin and for no minibtach discrimination are stored in the vectors __d_parameters__ and __simple_d_parameters__ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_d_1 = tf.Variable(tf.random_uniform([1, hidden_layer_size_d], minval=0, maxval=1, dtype=tf.float32))\n",
    "bias_d_1 = tf.Variable(tf.random_uniform([hidden_layer_size_d], minval=0, maxval=1, dtype=tf.float32))\n",
    "weight_d_2 = tf.Variable(tf.random_uniform([hidden_layer_size_d, hidden_layer_size_d], minval=0, maxval=1, dtype=tf.float32))\n",
    "bias_d_2 = tf.Variable(tf.random_uniform([hidden_layer_size_d], minval=0, maxval=1, dtype=tf.float32))\n",
    "weight_d_3 = tf.Variable(tf.random_uniform([hidden_layer_size_d, 1], minval=0, maxval=1, dtype=tf.float32))\n",
    "bias_d_3 = tf.Variable(tf.random_uniform([1], minval=0, maxval=1, dtype=tf.float32))\n",
    "\n",
    "mini_weight_1 = tf.Variable(tf.random_uniform([1], minval=0, maxval=1, dtype=tf.float32))\n",
    "mini_bias_1 = tf.Variable(tf.random_uniform([1], minval=0, maxval=1, dtype=tf.float32))\n",
    "\n",
    "mean_weight_1 = tf.Variable(tf.random_uniform([1], minval=0, maxval=1, dtype=tf.float32))\n",
    "mean_bias_1 = tf.Variable(tf.random_uniform([1], minval=0, maxval=1, dtype=tf.float32))\n",
    "\n",
    "mixing_weight1 = tf.Variable(tf.random_uniform([1], minval=0, maxval=1, dtype=tf.float32))\n",
    "mixing_weight2 = tf.Variable(tf.random_uniform([1], minval=0, maxval=1, dtype=tf.float32))\n",
    "\n",
    "d_parameters = [weight_d_1,bias_d_1, weight_d_2, bias_d_2,weight_d_3,\n",
    "                bias_d_3,mini_weight_1,mini_bias_1,mean_weight_1,mean_bias_1,mixing_weight1,mixing_weight2]\n",
    "\n",
    "simple_d_parameters = [weight_d_1,bias_d_1, weight_d_2, bias_d_2,weight_d_3, bias_d_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the parameters for $G$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_g_1 = tf.Variable(tf.random_uniform([1, hidden_layer_size_g], minval=0, maxval=1, dtype=tf.float32))\n",
    "bias_g_1 = tf.Variable(tf.random_uniform([hidden_layer_size_g], minval=0, maxval=1, dtype=tf.float32))\n",
    "weight_g_2 = tf.Variable(tf.random_uniform([hidden_layer_size_g, 1], minval=0, maxval=1, dtype=tf.float32))\n",
    "bias_g_2 = tf.Variable(tf.random_uniform([1], minval=0, maxval=1, dtype=tf.float32))\n",
    "\n",
    "g_parameters = [weight_g_1,bias_g_1, weight_g_2, bias_g_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the discriminators output, starting by defining place holders through which to feed the real and input samples. The actual defenitions of the losses are a bit more complex than in Appendix XYZ1 due to the fact we are carrying out multiple specifications in this single script. We first feed the real data into our range function __real_batch_l1__ and also calculate the minbatch mean __real_mean__. We first deal with the case that the sample is from the real distribution. We create a dictionary, __discriminator_dict_1__ with keys True and False, correspoding to whether we selected to have minibatch discrrimination or not. The values are the relevant discriminator acting on the data from the target distribution. We next deal with the case that sample is generated. Note the use of __reuse_variables()__ to ensure the same parameters are used. We then create another dicrtionary and do the equivilant steps to calculate the disciminators output in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dist_placeholder = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "generator_input_placeholder = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "with tf.variable_scope(\"Discrim\") as scope:\n",
    "    real_batch_l1 = minibatch_l1(real_dist_placeholder)\n",
    "    real_mean = tf.reduce_mean(real_dist_placeholder)\n",
    "    discriminator_dict_1 = {False: simple_discriminator(real_dist_placeholder,simple_d_parameters),\n",
    "                        True: discriminator(real_dist_placeholder, d_parameters, real_batch_l1,real_mean)}\n",
    "    d_output_real = discriminator_dict_1[minibatch_discrimination]\n",
    "    scope.reuse_variables()\n",
    "    fake_batch_l1 = minibatch_l1(generator(generator_input_placeholder, g_parameters))\n",
    "    fake_mean = tf.reduce_mean(generator(generator_input_placeholder, g_parameters))\n",
    "    discriminator_dict_2 = {False: simple_discriminator(generator(generator_input_placeholder, g_parameters),simple_d_parameters),\n",
    "                        True: discriminator(generator(generator_input_placeholder, g_parameters), d_parameters,fake_batch_l1,fake_mean)}\n",
    "    d_output_fake = discriminator_dict_2[minibatch_discrimination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use these variables to calculate losses. We use another dictionary here __objectives__ with keys 1 and 2 corresponding to which objective function we are going to use for the geneerator. The values of the dictionary are the corresponding losses for $G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "objectives = {1: tf.reduce_mean(tf.log(1-d_output_fake)) , 2: -tf.reduce_mean(tf.log(d_output_fake))}\n",
    "loss_d = tf.reduce_mean(-tf.log(d_output_real) - tf.log(1 - d_output_fake))\n",
    "loss_g = objectives[which_objective]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the train step, using a placeholder for the learning rate, which will be randomized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "train_g = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_g, var_list=g_parameters)\n",
    "train_d = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_d, var_list=d_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the learning rate, and create a matrix to store the ouputed sample, called __res_matrix__ and a vector to store the time taken, to allow us to see if the extra calculations involved in minibatch discirmination make a significant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_vec = np.random.uniform(0.000001,0.1,1)\n",
    "\n",
    "res_matrix = np.zeros((len(learning_rate_vec), batch_size))\n",
    "time_taken_out_vec = np.zeros((len(learning_rate_vec)))\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start the TensorFlow session and initialize all variables. We then generate the real samples (from the mixture distribution) by doing a coin flip for each one and then generating from the relevant Guassian. We feed this into the placeholders and carry out the train steps. After that loop we generate the final data for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for step in range(number_of_epochs):\n",
    "        generator_input = np.random.uniform(0, 1, (batch_size, 1))\n",
    "        # sample data\n",
    "        which = np.random.choice((0, 1), batch_size)  # bernoulli deciding which gaussian to sample from\n",
    "        means = which * real_mean_1 + (1 - which) * real_mean_2  # chooses mean_1 if which = 1\n",
    "        sds = which * real_sd_1 + (1 - which) * real_sd_2  # chooses sd_1 if which = 1\n",
    "        real_dist = np.random.normal(means, sds, batch_size)  # generate samples\n",
    "        real_dist = real_dist.reshape((batch_size, 1))\n",
    "\n",
    "        sess.run(train_d, feed_dict={real_dist_placeholder: real_dist,\n",
    "                                     generator_input_placeholder: generator_input,\n",
    "                                     learning_rate : learning_rate_vec[0]})\n",
    "        sess.run(train_g, feed_dict={real_dist_placeholder: real_dist,\n",
    "                                     generator_input_placeholder: generator_input,\n",
    "                                     learning_rate: learning_rate_vec[0]})\n",
    "\n",
    "    generator_input = np.random.uniform(0, 1, (batch_size, 1))\n",
    "    generated = sess.run(generator(generator_input, g_parameters))\n",
    "\n",
    "    res_matrix[0] = generated.reshape(batch_size)\n",
    "    time_taken_out_vec[0] = time.time()-start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we merge the data and export. We used different folders in the data directory for the four different groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dataframe = pd.DataFrame(data=res_matrix.astype(float))\n",
    "learning_rate_dataframe = pd.DataFrame(data=learning_rate_vec.astype(float))\n",
    "time_dataframe = pd.DataFrame(data=time_taken_out_vec.astype(float))\n",
    "\n",
    "output_dataframe1 = pd.concat([learning_rate_dataframe.reset_index(drop=True), time_dataframe], axis=1)\n",
    "output_dataframe = pd.concat([output_dataframe1.reset_index(drop=True), res_dataframe], axis=1)\n",
    "\n",
    "minibatch_dict = {True: 'minibatch', False : 'no_minibatch'}\n",
    "data_directory = '/data_output/objective_{}_{}'.format(which_objective,minibatch_dict[minibatch_discrimination])\n",
    "os.chdir(data_directory)\n",
    "\n",
    "with open(\"output.csv\", 'a') as f:\n",
    "    output_dataframe.to_csv(f, sep=',', header=False, float_format='%.9f', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
